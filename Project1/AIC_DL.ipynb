{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jiko0bSLC8Rt",
        "outputId": "651bec48-62a8-44f3-e5ab-ee2bf2d9ac6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------------------deep learning--------------------------------------------------------------------\n",
        "!pip install mne &> /dev/null\n",
        "!pip install skorch -U &> /dev/null\n",
        "!pip install mne-icalabel &> /dev/null\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import os\n",
        "import numpy as np\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from scipy.io import loadmat\n",
        "from mne.preprocessing import ICA\n",
        "from mne_icalabel import label_components\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pickle\n",
        "from skorch import NeuralNetClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from mne.decoding import CSP\n",
        "from mne.filter import filter_data\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mne.set_log_level(False)"
      ],
      "metadata": {
        "id": "gJYmwyjBDIvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbj_n = 4 #@param {type:\"integer\"}\n",
        "sbj_path = 'Subject ' + str(sbj_n)"
      ],
      "metadata": {
        "id": "OSSXOwyIDJ2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/EEG data'\n",
        "subj_folder = os.path.join(base_path, sbj_path)\n",
        "l = [file for file in os.listdir(subj_folder) if file.endswith('gdf')]"
      ],
      "metadata": {
        "id": "n8AKKKqFDMNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_1 = [file for file in os.listdir(subj_folder) if file.endswith('1.gdf')]\n",
        "type_2 = [file for file in os.listdir(subj_folder) if file.endswith('2.gdf')]"
      ],
      "metadata": {
        "id": "bLceaXPcDN1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smaller = min(len(type_1), len(type_2))\n",
        "type_1 = type_1[:smaller]\n",
        "type_2 = type_2[:smaller]"
      ],
      "metadata": {
        "id": "HtVCmdegDP5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constant\n",
        "ch_list = ['Fp1', 'Fp2', 'AF3', 'AF4', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FT7', 'FC3', 'FCz','FC4', 'FT8', 'T7', 'C3', 'Cz', 'C4', 'T8', 'TP7', 'CP3', 'CPz', 'CP4', 'TP8', 'P7', 'P3', 'Pz', 'P4', 'P8', 'O1', 'Oz', 'O2']\n",
        "# For epoching\n",
        "st = 0\n",
        "# For sub-epochs\n",
        "sub_dur = 2\n",
        "stride = 0.25\n",
        "# For cross-validation dataset\n",
        "n_splits = 4"
      ],
      "metadata": {
        "id": "FxILW2nSDRCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ds(pt, f):       # For BCIC Dataset\n",
        "  fp = os.path.join(pt, f)\n",
        "  print(\"File path: {}\".format(fp))\n",
        "  if fp.endswith('gdf'):\n",
        "    raw_data = mne.io.read_raw_gdf(fp, preload=True)\n",
        "  else:\n",
        "    raw_data = None\n",
        "  return raw_data\n",
        "\n",
        "def select_event(events_from_annot, event_dict, sfreq):\n",
        "  # specify needed event\n",
        "  l_ev = event_dict['769']\n",
        "  r_ev = event_dict['770']\n",
        "  x_ev = event_dict['33024']\n",
        "  end_trial = event_dict['800']\n",
        "  needed_event = [l_ev, r_ev, x_ev, end_trial]\n",
        "  # Remove unecessary last part\n",
        "  re = events_from_annot[::-1, 2].tolist()  # reverse\n",
        "  last_id = re.index(end_trial)             # Find index of last end signal\n",
        "  if last_id > 0:                           # Filter out last part\n",
        "    events_annot = events_from_annot[:-last_id]\n",
        "  else:\n",
        "    events_annot = events_from_annot[:]\n",
        "  # Filter put other event except the one used\n",
        "  mask = np.isin(events_annot[:, 2], needed_event)\n",
        "  filtered_events = events_annot[mask]\n",
        "  # Get durations\n",
        "  a = np.diff(filtered_events[:, 0])/sfreq\n",
        "  dur = round(np.mean(a[::2]),2)\n",
        "  print('Average duration of this session trials : ',dur)\n",
        "  return needed_event, dur\n",
        "\n",
        "def sub_epochs(epochs):\n",
        "  smaller_epochs = []\n",
        "  sf = epochs.info['sfreq']\n",
        "  for epoch in epochs:\n",
        "    data = epoch[np.newaxis,:,:]\n",
        "    # Calculate the number of smaller epochs that can be created\n",
        "    n_epochs = (data.shape[2] - (sub_dur * sf)) // (stride * sf) + 1\n",
        "    for i in range(int(n_epochs)):\n",
        "        start_sample = int(i * (stride * sf))\n",
        "        end_sample = start_sample + int(sub_dur * sf)\n",
        "        smaller_epoch_data = data[:, :, start_sample:end_sample]\n",
        "\n",
        "        # Create a new Epoch object with the smaller epoch data\n",
        "        smaller_epoch = mne.EpochsArray(smaller_epoch_data, info=epochs.info)\n",
        "        smaller_epochs.append(smaller_epoch)\n",
        "  # Combine all the smaller epochs into a single Epochs object\n",
        "  smaller_epochs = mne.epochs.concatenate_epochs(smaller_epochs)\n",
        "  return smaller_epochs\n",
        "\n",
        "def create_label(size, lbl):\n",
        "  return np.full(size, lbl)\n",
        "\n",
        "def epoch_array(epoch_l, epoch_r, epoch_x):\n",
        "  mini_ep_l = sub_epochs(epoch_l)   # Create sub epochs\n",
        "  mini_ep_r = sub_epochs(epoch_r)\n",
        "  mini_ep_x = sub_epochs(epoch_x)\n",
        "  ec_l = len(mini_ep_l)/8       # Calculate sub-epochs per trial\n",
        "  ec_r = len(mini_ep_r)/8\n",
        "  ec_x = len(mini_ep_x)/8\n",
        "  ep_l = mini_ep_l.get_data(copy=True)   # Turn to array\n",
        "  ep_r = mini_ep_r.get_data(copy=True)\n",
        "  ep_x = mini_ep_x.get_data(copy=True)\n",
        "  lbl_l = create_label(ep_l.shape[0], 1)    # Create Labels\n",
        "  lbl_r = create_label(ep_r.shape[0], 2)\n",
        "  lbl_x = create_label(ep_x.shape[0], 0)\n",
        "  # Combine arrays\n",
        "  epoch_data = np.concatenate((ep_l, ep_r, ep_x),axis=0)\n",
        "  label_data = np.concatenate((lbl_l, lbl_r, lbl_x),axis=0)\n",
        "  return epoch_data, label_data\n",
        "\n",
        "def balance_dataset(X, y):\n",
        "  unique, counts = np.unique(y, return_counts=True)\n",
        "  groups = np.hstack((np.repeat(np.arange(8), ([int(counts[1]/8)]*8)), np.repeat(np.arange(8), ([int(counts[2]/8)]*8)), np.repeat(np.arange(8), ([int(counts[0]/8)]*8))))\n",
        "  # Randomly sample from class 0, get indices\n",
        "  balanced_indices = []\n",
        "  for group_val in range(8):\n",
        "      class_2_indices = np.where((groups == group_val) & (y == 0))[0]\n",
        "      selected_indices = np.random.choice(class_2_indices, size=int(counts[1]/8), replace=False)\n",
        "      balanced_indices.extend(selected_indices)\n",
        "  balanced_indices.sort()\n",
        "  balanced_X = np.concatenate([X[y != 0], X[balanced_indices]])\n",
        "  balanced_y = np.concatenate([y[y != 0], y[balanced_indices]])\n",
        "  unique2, counts2 = np.unique(balanced_y, return_counts=True)\n",
        "  new_groups = np.hstack((np.repeat(np.arange(8), ([int(counts2[1]/8)]*8)), np.repeat(np.arange(8), ([int(counts2[2]/8)]*8)), np.repeat(np.arange(8), ([int(counts2[0]/8)]*8))))\n",
        "  return balanced_X, balanced_y, new_groups\n",
        "\n",
        "def save_epoch(X, y, file_name):\n",
        "  # Names\n",
        "  epoch_folder = subj_folder + '/Epoch'\n",
        "  rec_name = file_name[:-4] + '_data.txt'\n",
        "  lbl_name = file_name[:-4] + '_label.txt'\n",
        "  # Path\n",
        "  ep_file = os.path.join(epoch_folder, rec_name)\n",
        "  lbl_file = os.path.join(epoch_folder, lbl_name)\n",
        "  arr_reshaped = X.reshape(X.shape[0], -1)\n",
        "  np.savetxt(ep_file, arr_reshaped)\n",
        "  np.savetxt(lbl_file, y)"
      ],
      "metadata": {
        "id": "JUnc13jPDSXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_list: list):\n",
        "  combined_set = []\n",
        "  for data in file_list:\n",
        "    x = load_ds(subj_folder, data)    # Load data\n",
        "    combined_set.append(x)\n",
        "  raw_cat = mne.concatenate_raws(combined_set)\n",
        "  channel_mapping = {old_name: new_name for old_name, new_name in zip(raw_cat.ch_names, ch_list)}   # Remap channels\n",
        "  raw_cat.rename_channels(channel_mapping)\n",
        "  montage = mne.channels.make_standard_montage('standard_1020')\n",
        "  _ = raw_cat.set_montage(montage)\n",
        "  events_from_annot, event_dict = mne.events_from_annotations(raw_cat)\n",
        "  needed_event, dur = select_event(events_from_annot, event_dict, raw_cat.info['sfreq'])\n",
        "  return raw_cat, needed_event, dur, events_from_annot"
      ],
      "metadata": {
        "id": "0wyiuOLlDZy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in type_1:\n",
        "  x = load_ds(subj_folder, data)    # Load data\n",
        "  events_from_annot, event_dict = mne.events_from_annotations(x)\n",
        "  needed_event, dur = select_event(events_from_annot, event_dict, x.info['sfreq'])\n",
        "  print(needed_event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsGAyjd2DbiF",
        "outputId": "09798f54-86a6-4e91-9578-501079d6b7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File path: /content/drive/MyDrive/EEG data/Subject 4/record-[2024.03.10]_S9_1.gdf\n",
            "Average duration of this session trials :  6.21\n",
            "[3, 4, 2, 5]\n",
            "File path: /content/drive/MyDrive/EEG data/Subject 4/record-[2024.03.10]_S7_1.gdf\n",
            "Average duration of this session trials :  6.21\n",
            "[3, 4, 2, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(file_list: list):\n",
        "  combined_set = []\n",
        "  combined_labels = []\n",
        "  combined_groups = []\n",
        "  for data in file_list:\n",
        "    x = load_ds(subj_folder, data)    # Load data\n",
        "    events_from_annot, event_dict = mne.events_from_annotations(x)\n",
        "    # Event select\n",
        "    needed_event, dur = select_event(events_from_annot, event_dict, x.info['sfreq'])\n",
        "    x = x.set_eeg_reference(\"average\")\n",
        "    print(needed_event)\n",
        "    # Load data into epochs\n",
        "    #epoch_base = mne.Epochs(x.copy().crop(tmin=10-dur, tmax=10), events_from_annot)\n",
        "\n",
        "    #--------------------origin--------------------------\n",
        "    epoch_l = mne.Epochs(x, events_from_annot, event_id=needed_event[0], tmin=st, tmax=st+dur, baseline=None, preload=True)\n",
        "    epoch_r = mne.Epochs(x, events_from_annot, event_id=needed_event[1], tmin=st, tmax=st+dur, baseline=None, preload=True)\n",
        "    epoch_x = mne.Epochs(x, events_from_annot, event_id=needed_event[2], tmin=st, tmax=st+dur, baseline=None, preload=True)\n",
        "    #--------------------origin--------------------------\n",
        "\n",
        "    # epoch_l = mne.Epochs(x, events_from_annot, event_id=needed_event[1], tmin=st, tmax=st+dur, baseline=None, preload=True)\n",
        "    # epoch_r = mne.Epochs(x, events_from_annot, event_id=needed_event[3], tmin=st, tmax=st+dur, baseline=None, preload=True)\n",
        "    # epoch_x = mne.Epochs(x, events_from_annot, event_id=needed_event[2], tmin=st, tmax=st+dur, baseline=None, preload=True)\n",
        "\n",
        "    if len(epoch_x) > (len(epoch_l)+len(epoch_r)):\n",
        "      epoch_x.drop([-1])    # Remove last epoch since it's\n",
        "    # Bandpass filter\n",
        "    #epoch_base.filter(l_freq=1, h_freq=40)\n",
        "\n",
        "    # retain alpha power band ?\n",
        "    epoch_l.filter(l_freq=6, h_freq=14)\n",
        "    epoch_r.filter(l_freq=6, h_freq=14)\n",
        "    epoch_x.filter(l_freq=6, h_freq=14)\n",
        "    X, y = epoch_array(epoch_l, epoch_r, epoch_x)\n",
        "\n",
        "    new_X, new_y, current_group = balance_dataset(X, y)\n",
        "    save_epoch(new_X, new_y, data)\n",
        "    combined_set.append(new_X)\n",
        "    combined_labels.append(new_y)\n",
        "    combined_groups.append(current_group)\n",
        "\n",
        "    # without balance data\n",
        "    # ------------------------------------------------------\n",
        "    # unique, counts = np.unique(y, return_counts=True)\n",
        "    # groups = np.hstack((np.repeat(np.arange(8), ([int(counts[1]/8)]*8)), np.repeat(np.arange(8), ([int(counts[2]/8)]*8)), np.repeat(np.arange(8), ([int(counts[0]/8)]*8))))\n",
        "    # save_epoch(X, y, data)\n",
        "    # combined_set.append(X)\n",
        "    # combined_labels.append(y)\n",
        "    # combined_groups.append(groups)\n",
        "    # ------------------------------------------------------\n",
        "\n",
        "  if len(combined_set) > 1:\n",
        "    result_X = np.vstack(combined_set)\n",
        "    result_y = np.concatenate(combined_labels)\n",
        "    result_group = np.concatenate(combined_groups)\n",
        "  else:\n",
        "    result_X = combined_set[0]\n",
        "    result_y = combined_labels[0]\n",
        "    result_group = combined_groups[0]\n",
        "  return result_X, result_y, result_group"
      ],
      "metadata": {
        "id": "HmSMSFB6DdB6"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import (\n",
        "    GroupKFold,\n",
        "    KFold,\n",
        "    StratifiedGroupKFold,\n",
        "    StratifiedKFold,\n",
        "    StratifiedShuffleSplit\n",
        ")\n",
        "\n",
        "cmap_data = plt.cm.Paired\n",
        "cmap_cv = plt.cm.coolwarm"
      ],
      "metadata": {
        "id": "CUJc_TP1tJjg"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Type1"
      ],
      "metadata": {
        "id": "i9qA9Q1O8Xh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds, lbl, grouping = create_dataset(type_1)"
      ],
      "metadata": {
        "id": "2DzjT99XtLkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ds))\n",
        "print(len(lbl))\n",
        "print(len(grouping))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfggzfhc3Azd",
        "outputId": "2df49d33-1da2-4c18-91f4-4d6012bd2972"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1088\n",
            "1088\n",
            "1088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedGroupKFold(n_splits)"
      ],
      "metadata": {
        "id": "Tps9Rba7tPde"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from scipy import signal\n",
        "from skorch.callbacks import Checkpoint, EpochScoring, EarlyStopping"
      ],
      "metadata": {
        "id": "YEnRPzvBtZs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 2\n",
        "sf = 128\n",
        "cls_n = 3"
      ],
      "metadata": {
        "id": "A9B4DamctdBX"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/High-East/BCI-ToolBox/blob/master/models/EEGNet/EEGNet.py\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, in_chn, n_cls, input_ts, f1=8, f2=16, d=2, drop_prob=0.5):\n",
        "        super(EEGNet, self).__init__()\n",
        "\n",
        "        self.F1 = f1   # High Frequency pattern\n",
        "        self.F2 = f2   # Lower Frequency patter\n",
        "        self.D = d    # Dilation (?), spatial?\n",
        "        #\n",
        "        self.kernel_l = math.ceil(sf/2)\n",
        "        self.chn = in_chn\n",
        "        self.cls = n_cls\n",
        "        self.drop_prob = drop_prob\n",
        "        self.tp = input_ts\n",
        "\n",
        "        # Spectral\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, self.F1, (1, self.kernel_l), padding=(0, math.ceil(self.kernel_l//2)), bias=False),\n",
        "            nn.BatchNorm2d(self.F1)\n",
        "        )\n",
        "\n",
        "        # Spectral-specific Spatial\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(self.F1, self.D*self.F1, (self.chn, 1), groups=self.F1, bias=False),\n",
        "            nn.BatchNorm2d(self.D*self.F1),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_prob)\n",
        "        )\n",
        "\n",
        "        # Temporal\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(self.D*self.F1, self.D*self.F1, (1, math.ceil(self.kernel_l//4)), padding=(0, 8), groups=self.D*self.F1, bias=False),\n",
        "            nn.Conv2d(self.D*self.F1, self.F2, (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(self.F2),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_prob)\n",
        "        )\n",
        "\n",
        "        #self.classifier = nn.Linear(math.ceil(self.kernel_l/4)* math.ceil(self.tp//32), self.cls, bias=True)\n",
        "        self.classifier = nn.Linear(self.F2 * math.ceil(self.tp//32), self.cls, bias=True)\n",
        "        #self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, self.F2*math.ceil(self.tp//32))\n",
        "        x = self.classifier(x)\n",
        "        #x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eTKXcQSYte3M"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sf = t * sf"
      ],
      "metadata": {
        "id": "dFO0ooBStgpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 200\n",
        "# learn_r = 0.001\n",
        "learn_r = 0.0005\n",
        "n_batch = 32"
      ],
      "metadata": {
        "id": "2sB_zvW5tiAH"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_resample = signal.resample(ds, new_sf, axis=2)\n",
        "y_resample = lbl.astype(np.int64)\n",
        "x_resample = np.expand_dims(x_resample, axis=1).astype(np.float32)"
      ],
      "metadata": {
        "id": "qikLvlO5tjMj"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNetClassifier(\n",
        "    EEGNet,\n",
        "    module__in_chn=x_resample.shape[-2],\n",
        "    module__n_cls=cls_n,\n",
        "    module__input_ts=x_resample.shape[-1],\n",
        "    criterion = torch.nn.CrossEntropyLoss(),\n",
        "    optimizer = torch.optim.AdamW,\n",
        "    iterator_train__shuffle=True,\n",
        "    batch_size = n_batch,\n",
        "    callbacks=[\n",
        "        EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
        "        Checkpoint(monitor='valid_loss_best'),  # save based on validation loss\n",
        "        #EarlyStopping(patience=50, monitor='valid_loss')\n",
        "    ],\n",
        "    max_epochs=n_epoch,\n",
        "    lr=learn_r,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")"
      ],
      "metadata": {
        "id": "0ZHfx5qntlLg"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(net, x_resample, y_resample, groups=grouping, cv=cv, n_jobs=None)"
      ],
      "metadata": {
        "id": "dip1cO8itmbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross-validated accuracy scores:\", scores)\n",
        "print(\"Mean accuracy:\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViRRX0tGtnft",
        "outputId": "674d318c-cb6a-4743-dd15-56aa4b13ced1"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy scores: [0.40073529 0.25       0.25       0.36029412]\n",
            "Mean accuracy: 0.31525735294117646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Shuffle"
      ],
      "metadata": {
        "id": "eu37FQX6t4Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv2 = StratifiedShuffleSplit(n_splits)"
      ],
      "metadata": {
        "id": "i8DFF3Zgt3mI"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores2 = cross_val_score(net, x_resample, y_resample, groups=grouping, cv=cv2, n_jobs=None)"
      ],
      "metadata": {
        "id": "r0tE_Ifvty_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross-validated accuracy scores:\", scores2)\n",
        "print(\"Mean accuracy:\", np.mean(scores2))"
      ],
      "metadata": {
        "id": "opD74p-Rt9mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb556b18-0937-493e-a067-ca48ab6222b1"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy scores: [0.69724771 0.26605505 0.25688073 0.6146789 ]\n",
            "Mean accuracy: 0.45871559633027525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Type2"
      ],
      "metadata": {
        "id": "iOuCactP8esz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds2, lbl2, grouping2 = create_dataset(type_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Qf-sDzj8iv8",
        "outputId": "630704f7-d2b7-4d60-d2bf-0ee1e7fa0687"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File path: /content/drive/MyDrive/EEG data/Subject 4/record-[2024.03.10]_S8_2.gdf\n",
            "Average duration of this session trials :  6.27\n",
            "[4, 5, 2, 6]\n",
            "File path: /content/drive/MyDrive/EEG data/Subject 4/record-[2024.03.10]_S10_2.gdf\n",
            "Average duration of this session trials :  6.28\n",
            "[4, 5, 2, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedGroupKFold(n_splits)"
      ],
      "metadata": {
        "id": "pvbV3WTq8jFq"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_r = 0.01"
      ],
      "metadata": {
        "id": "esUZBmJs9ctD"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_resample = signal.resample(ds2, new_sf, axis=2)\n",
        "y_resample = lbl2.astype(np.int64)\n",
        "x_resample = np.expand_dims(x_resample, axis=1).astype(np.float32)"
      ],
      "metadata": {
        "id": "d4H96X7D8llk"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNetClassifier(\n",
        "    EEGNet,\n",
        "    module__in_chn=x_resample.shape[-2],\n",
        "    module__n_cls=cls_n,\n",
        "    module__input_ts=x_resample.shape[-1],\n",
        "    criterion = torch.nn.CrossEntropyLoss(),\n",
        "    optimizer = torch.optim.AdamW,\n",
        "    iterator_train__shuffle=True,\n",
        "    batch_size = n_batch,\n",
        "    callbacks=[\n",
        "        EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
        "        Checkpoint(monitor='valid_loss_best'),  # save based on validation loss\n",
        "        #EarlyStopping(patience=50, monitor='valid_loss')\n",
        "    ],\n",
        "    max_epochs=n_epoch,\n",
        "    lr=learn_r,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")"
      ],
      "metadata": {
        "id": "FtMmjwEW8n1f"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(net, x_resample, y_resample, groups=grouping2, cv=cv, n_jobs=None)"
      ],
      "metadata": {
        "id": "UAoCfJud8pGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross-validated accuracy scores:\", scores)\n",
        "print(\"Mean accuracy:\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn643Iz18qao",
        "outputId": "f16feb65-2dc2-456f-ecaa-1fff5be1a65e"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy scores: [0.375      0.41666667 0.3287037  0.36111111]\n",
            "Mean accuracy: 0.3703703703703704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Shuffle"
      ],
      "metadata": {
        "id": "0K1TrR8d8uJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv2 = StratifiedShuffleSplit(n_splits)"
      ],
      "metadata": {
        "id": "8tYp5kdG8sDC"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores2 = cross_val_score(net, x_resample, y_resample, groups=grouping2, cv=cv2, n_jobs=None)"
      ],
      "metadata": {
        "id": "wZju9Kxg80bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross-validated accuracy scores:\", scores2)\n",
        "print(\"Mean accuracy:\", np.mean(scores2))\n",
        "# --------------------------------------------------------------------deep learning--------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfUPWukR81ru",
        "outputId": "8413c854-7c4d-4156-959d-7cded8c893d2"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy scores: [0.4137931  0.8045977  0.42528736 0.8045977 ]\n",
            "Mean accuracy: 0.6120689655172414\n"
          ]
        }
      ]
    }
  ]
}